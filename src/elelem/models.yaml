# Model definitions with capabilities and pricing for all supported providers
# Also includes provider endpoint configurations

# Provider configurations
providers:
  openai:
    endpoint: https://api.openai.com/v1
    default_params: {}
    
  groq:
    endpoint: https://api.groq.com/openai/v1
    default_params:
      service_tier: auto
      
  deepinfra:
    endpoint: https://api.deepinfra.com/v1/openai
    default_params: {}
    
  scaleway:
    endpoint: https://api.scaleway.ai/v1
    default_params: {}
    
  openrouter:
    endpoint: https://openrouter.ai/api/v1
    headers:
      HTTP-Referer: https://github.com/guybrush1984/Elelem
      X-Title: Elelem
    default_params: {}
    extra_body:
      usage:
        include: true
  
  parasail:
    endpoint: https://api.parasail.io/v1
    default_params: {}
  
  fireworks:
    endpoint: https://api.fireworks.ai/inference/v1
    default_params:
      stream: true
    max_tokens_default: 16384
  
  # Infrastructure providers - force specific providers via OpenRouter
  throughput@openrouter:
    base_provider: openrouter
    endpoint: https://openrouter.ai/api/v1
    headers:
      HTTP-Referer: https://github.com/guybrush1984/Elelem
      X-Title: Elelem
    default_params: {}
    extra_body:
      usage:
        include: true
      provider:
        only: ["cerebras", "groq", "together", "deepinfra", "novita", "fireworks"]
        sort: "throughput"
        allow_fallbacks: false

  # Infrastructure providers - force specific providers via OpenRouter
  cost@openrouter:
    base_provider: openrouter
    endpoint: https://openrouter.ai/api/v1
    headers:
      HTTP-Referer: https://github.com/guybrush1984/Elelem
      X-Title: Elelem
    default_params: {}
    extra_body:
      usage:
        include: true
      provider:
        only: ["cerebras", "groq", "together", "deepinfra", "novita", "fireworks"]
        sort: "price"
        allow_fallbacks: false        

models:
  # OpenAI Models (Standard tier pricing)
  "openai:gpt-4.1":
    provider: openai
    model_id: gpt-4.1
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 2.00  # $2.00 per 1M input tokens
      output_cost_per_1m: 8.00  # $8.00 per 1M output tokens
      currency: USD

  "openai:gpt-4.1-mini":
    provider: openai
    model_id: gpt-4.1-mini
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.40  # $0.40 per 1M input tokens
      output_cost_per_1m: 1.60  # $1.60 per 1M output tokens
      currency: USD

  "openai:gpt-5":
    provider: openai
    model_id: gpt-5
    capabilities:
      supports_json_mode: true
      supports_temperature: false  # Only supports default temperature (1)
      supports_system: true
    cost:
      input_cost_per_1m: 1.25
      output_cost_per_1m: 10.00
      currency: USD

  "openai:gpt-5-mini":
    provider: openai
    model_id: gpt-5-mini
    capabilities:
      supports_json_mode: true
      supports_temperature: false  # Only supports default temperature (1)
      supports_system: true
    cost:
      input_cost_per_1m: 0.25
      output_cost_per_1m: 2.00
      currency: USD

  "openai:o3":
    provider: openai
    model_id: o3
    capabilities:
      supports_json_mode: true
      supports_temperature: false  # o-series models don't support temperature
    cost:
      input_cost_per_1m: 2.00
      output_cost_per_1m: 8.00
      currency: USD

  "openai:o3-mini":
    provider: openai
    model_id: o3-mini
    capabilities:
      supports_json_mode: true
      supports_temperature: false
      supports_system: true
    cost:
      input_cost_per_1m: 1.10
      output_cost_per_1m: 4.40
      currency: USD

  # GROQ Models
  "groq:openai/gpt-oss-120b":
    provider: groq
    model_id: openai/gpt-oss-120b  # GROQ's model ID format
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.15
      output_cost_per_1m: 0.75
      currency: USD

  "groq:openai/gpt-oss-20b":
    provider: groq
    model_id: openai/gpt-oss-20b
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.10
      output_cost_per_1m: 0.50
      currency: USD

  "groq:moonshotai/kimi-k2-instruct":
    provider: groq
    model_id: moonshotai/kimi-k2-instruct
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 1.00
      output_cost_per_1m: 3.00
      currency: USD

  "groq:meta-llama/llama-4-maverick-17b-128e-instruct":
    provider: groq
    model_id: meta-llama/llama-4-maverick-17b-128e-instruct
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.20
      output_cost_per_1m: 0.60
      currency: USD

  "groq:meta-llama/llama-4-scout-17b-16e-instruct":
    provider: groq
    model_id: meta-llama/llama-4-scout-17b-16e-instruct
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.11
      output_cost_per_1m: 0.34
      currency: USD

  # DeepInfra Models - CRITICAL: ALL supports_json_mode = false
  "deepinfra:openai/gpt-oss-120b":
    provider: deepinfra
    model_id: openai/gpt-oss-120b
    capabilities:
      supports_json_mode: false  # DeepInfra doesn't support response_format
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.09
      output_cost_per_1m: 0.45
      currency: USD

  "deepinfra:openai/gpt-oss-20b":
    provider: deepinfra
    model_id: openai/gpt-oss-20b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.04
      output_cost_per_1m: 0.16
      currency: USD

  "deepinfra:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8":
    provider: deepinfra
    model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.08  # Need to verify - using Scout pricing as placeholder
      output_cost_per_1m: 0.30
      currency: USD

  "deepinfra:meta-llama/Llama-4-Scout-17B-16E-Instruct":
    provider: deepinfra
    model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.08
      output_cost_per_1m: 0.30
      currency: USD

  "deepinfra:moonshotai/Kimi-K2-Instruct":
    provider: deepinfra
    model_id: moonshotai/Kimi-K2-Instruct
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.50
      output_cost_per_1m: 2.00
      currency: USD

  "deepinfra:deepseek-ai/DeepSeek-R1-0528":
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-R1-0528
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.50
      output_cost_per_1m: 2.15
      currency: USD

  "deepinfra:deepseek-ai/DeepSeek-V3.1":
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD

  "deepinfra:deepseek-ai/DeepSeek-V3.1-think":
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD

  "deepinfra:deepseek-ai/DeepSeek-R1-Distill-Llama-70B":
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.10
      output_cost_per_1m: 0.40
      currency: USD

  "deepinfra:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.075
      output_cost_per_1m: 0.15
      currency: USD

  # Scaleway Models (European data centers, EUR pricing converted to USD ~1.07 rate)
  "scaleway:gpt-oss-120b":
    provider: scaleway
    model_id: gpt-oss-120b
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.16   # €0.15 → $0.16 USD
      output_cost_per_1m: 0.64  # €0.60 → $0.64 USD
      currency: USD

  "scaleway:gemma-3-27b-it":
    provider: scaleway
    model_id: gemma-3-27b-it
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.27   # €0.25 → $0.27 USD
      output_cost_per_1m: 0.53  # €0.50 → $0.53 USD
      currency: USD

  "scaleway:mistral-small-3.2-24b-instruct-2506":
    provider: scaleway
    model_id: mistral-small-3.2-24b-instruct-2506
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.16   # €0.15 → $0.16 USD
      output_cost_per_1m: 0.37  # €0.35 → $0.37 USD
      currency: USD

  "scaleway:qwen3-235b-a22b-instruct-2507":
    provider: scaleway
    model_id: qwen3-235b-a22b-instruct-2507
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.80   # €0.75 → $0.80 USD
      output_cost_per_1m: 2.40  # €2.25 → $2.40 USD
      currency: USD

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:openai/gpt-oss-20b":
    provider: openrouter
    model_id: openai/gpt-oss-20b  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:openai/gpt-oss-120b":
    provider: openrouter
    model_id: openai/gpt-oss-120b  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:openai/gpt-oss-120b-nitro":
    provider: openrouter
    model_id: openai/gpt-oss-120b:nitro  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:anthropic/claude-sonnet-4":
    provider: openrouter
    model_id: anthropic/claude-sonnet-4  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:mistralai/mixtral-8x22b-instruct":
    provider: openrouter
    model_id: mistralai/mixtral-8x22b-instruct  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # OpenRouter Models (Automatic provider routing with runtime pricing)
  "openrouter:mistralai/mistral-large-2411":
    provider: openrouter
    model_id: mistralai/mistral-large-2411  # Use :floor suffix to enforce price-first routing
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime  # Costs determined at runtime from OpenRouter response headers

  # Infrastructure provider-specific model
  "throughput@openrouter:openai/gpt-oss-120b":
    provider: throughput@openrouter
    model_id: openai/gpt-oss-120b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime 

 # Infrastructure provider-specific model
  "cost@openrouter:openai/gpt-oss-120b":
    provider: cost@openrouter
    model_id: openai/gpt-oss-120b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime

  # Infrastructure provider-specific model
  "throughput@openrouter:openai/gpt-oss-20b":
    provider: throughput@openrouter
    model_id: openai/gpt-oss-20b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime 

 # Infrastructure provider-specific model
  "cost@openrouter:openai/gpt-oss-20b":
    provider: cost@openrouter
    model_id: openai/gpt-oss-20b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime    

  # Infrastructure provider-specific model
  "throughput@openrouter:deepseek/deepseek-3.1":
    provider: throughput@openrouter
    model_id: deepseek/deepseek-chat-v3.1
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime 

 # Infrastructure provider-specific model
  "cost@openrouter:deepseek/deepseek-3.1":
    provider: cost@openrouter
    model_id: deepseek/deepseek-chat-v3.1
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost: runtime

  # Parasail Models
  "parasail:deepseek-3.1":
    provider: parasail
    model_id: parasail-deepseek-31
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.64
      output_cost_per_1m: 1.65
      currency: USD

  "parasail:gpt-oss-120b":
    provider: parasail
    model_id: parasail-gpt-oss-120b
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.15
      output_cost_per_1m: 0.60
      currency: USD

  # Fireworks Models
  "fireworks:deepseek-3.1":
    provider: fireworks
    model_id: accounts/fireworks/models/deepseek-v3p1
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.56
      output_cost_per_1m: 1.68
      currency: USD

  "fireworks:gpt-oss-120b":
    provider: fireworks
    model_id: accounts/fireworks/models/gpt-oss-120b
    capabilities:
      supports_json_mode: trueok
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.15
      output_cost_per_1m: 0.60
      currency: USD

  "fireworks:gpt-oss-20b":
    provider: fireworks
    model_id: accounts/fireworks/models/gpt-oss-20b
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.07
      output_cost_per_1m: 0.30
      currency: USD

  # ================================================================
  # VIRTUAL MODELS
  # ================================================================

  # Virtual model combining multiple gpt-oss-120b providers for resilience
  "virtual:gpt-oss-120b":
    timeout: 180s
    candidates:
      - model: "parasail:gpt-oss-120b"
        timeout: 60s   # Try Parasail first
      - model: "fireworks:gpt-oss-120b"
        timeout: 90s   # Fireworks fallback with more time
      - model: "groq:openai/gpt-oss-120b"
        timeout: 120s  # Groq as final fallback

  # Virtual model combining multiple gpt-oss-20b providers
  "virtual:gpt-oss-20b":
    timeout: 180s
    candidates:
      - model: "fireworks:gpt-oss-20b"
        timeout: 60s
      - model: "groq:openai/gpt-oss-20b"
        timeout: 90s
      - model: "deepinfra:openai/gpt-oss-20b"
        timeout: 120s

  # Virtual model for DeepSeek variants across providers
  "virtual:deepseek-v3.1":
    timeout: 200s
    candidates:
      - model: "deepinfra:deepseek-ai/DeepSeek-V3.1"
        timeout: 120s
      - model: "parasail:deepseek-31"
        timeout: 60s
      - model: "fireworks:deepseek-v3p1"
        timeout: 90s

  # Cost-optimized routing (cheapest first)
  "virtual:gpt-oss-120b-cheap":
    timeout: 240s
    candidates:
      - model: "parasail:gpt-oss-120b"     # $0.15/$0.60
        timeout: 60s
      - model: "fireworks:gpt-oss-120b"    # $0.15/$0.60
        timeout: 90s
      - model: "deepinfra:openai/gpt-oss-120b"  # $0.09/$0.45 - cheapest but last due to no JSON support
        timeout: 120s

  # Speed-optimized routing (fastest first) - shorter timeouts for speed
  "virtual:gpt-oss-120b-fast":
    timeout: 120s
    candidates:
      - model: "groq:openai/gpt-oss-120b"  # Usually fastest
        timeout: 30s   # Very short timeout to force quick failover
      - model: "fireworks:gpt-oss-120b"    # Good speed
        timeout: 60s
      - model: "parasail:gpt-oss-120b"     # Fallback
        timeout: 90s

    
