# DeepInfra provider configuration and models
# CRITICAL: ALL supports_json_mode = false
provider:
  endpoint: https://api.deepinfra.com/v1/openai
  default_params: {}

models:
  "deepinfra:openai/gpt-oss-120b?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_120b"
      model_configuration: "reasoning=$reasoning"
    provider: deepinfra
    model_id: openai/gpt-oss-120b
    capabilities:
      supports_json_mode: false  # DeepInfra doesn't support response_format
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.09
      output_cost_per_1m: 0.45
      currency: USD
    default_params:
      reasoning_effort: $reasoning
      stream: true


  "deepinfra:openai/gpt-oss-20b?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_20b"
      model_configuration: "reasoning=$reasoning"
    provider: deepinfra
    model_id: openai/gpt-oss-20b
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.04
      output_cost_per_1m: 0.16
      currency: USD
    default_params:
      reasoning_effort: $reasoning  
      stream: true


  "deepinfra:meta-llama/llama-4-maverick-17b":
    metadata:
      model_reference: "meta_llama4_maverick"
    provider: deepinfra
    model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.08  # Need to verify - using Scout pricing as placeholder
      output_cost_per_1m: 0.30
      currency: USD

  "deepinfra:meta-llama/llama-4-scout-17b":
    metadata:
      model_reference: "meta_llama4_scout"
    provider: deepinfra
    model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.08
      output_cost_per_1m: 0.30
      currency: USD

  "deepinfra:deepseek/deepseek-3.1":
    metadata:
      model_reference: "deepseek_v31"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD
    default_params:
      stream: true


  "deepinfra:deepseek/deepseek-3.1?reasoning":
    metadata:
      model_reference: "deepseek_v31"
      model_configuration: "reasoning=$reasoning"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1
    timeout: 300
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD
    default_params:
      stream: true
      reasoning_effort: medium

  "deepinfra:deepseek/deepseek-3.1-terminus":
    metadata:
      model_reference: "deepseek_v31-terminus"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1-Terminus
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD
    default_params:
      stream: true

  "deepinfra:deepseek/deepseek-3.1-terminus?reasoning":
    metadata:
      model_reference: "deepseek_v31-terminus"
      model_configuration: "reasoning=$reasoning"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.1-Terminus
    timeout: 300
    capabilities:
      supports_json_mode: false
      supports_temperature: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 1.00
      currency: USD
    default_params:
      stream: true
      reasoning_effort: medium

  "deepinfra:qwen/qwen3-30b-a3b?reasoning=@[none,high]":
    metadata:
      model_reference: "alibaba_qwen3_30b_a3b"
      model_configuration: "reasoning=$reasoning"
    provider: deepinfra
    model_id: Qwen/Qwen3-30B-A3B
    capabilities:
      supports_json_mode: false  
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.08
      output_cost_per_1m: 0.29
      currency: USD
    default_params:
      stream: true
      reasoning_effort: $reasoning

  "deepinfra:mistral/mistral-small":
    metadata:
      model_reference: "mistral_small_32"
    provider: deepinfra
    model_id: mistralai/Mistral-Small-3.2-24B-Instruct-2506
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.075
      output_cost_per_1m: 0.20
      currency: USD

  "deepinfra:deepseek/deepseek-3.2":
    metadata:
      model_reference: "deepseek_v32"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.2
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.27
      output_cost_per_1m: 0.40
      currency: USD
    default_params:
      stream: true

  "deepinfra:deepseek/deepseek-3.2?reasoning":
    metadata:
      model_reference: "deepseek_v32"
      model_configuration: "reasoning"
    provider: deepinfra
    model_id: deepseek-ai/DeepSeek-V3.2
    timeout: 300
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.27
      output_cost_per_1m: 0.40
      currency: USD
    default_params:
      stream: true
      reasoning_effort: medium

  "deepinfra:moonshotai/kimi-k2-instruct":
    metadata:
      model_reference: "moonshot_kimi_k2"
    provider: deepinfra
    model_id: moonshotai/Kimi-K2-Instruct-0905
    capabilities:
      supports_json_mode: false
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.50
      output_cost_per_1m: 2.00
      currency: USD

