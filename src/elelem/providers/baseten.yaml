# Baseten provider configuration and models
# API docs: https://docs.baseten.co/
provider:
  endpoint: https://inference.baseten.co/v1
  default_params:
    stream: true

models:
  # DeepSeek V3.2
  "baseten:deepseek/deepseek-3.2":
    metadata:
      model_reference: "deepseek_v32"
    provider: baseten
    model_id: deepseek-ai/DeepSeek-V3.2
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 0.45
      currency: USD

  "baseten:deepseek/deepseek-3.2?reasoning":
    metadata:
      model_reference: "deepseek_v32"
      model_configuration: "reasoning"
    provider: baseten
    model_id: deepseek-ai/DeepSeek-V3.2
    timeout: 300
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 0.45
      currency: USD
    extra_body:
      chat_template_args:
        enable_thinking: true

  # GPT-OSS 120B with reasoning parameter
  "baseten:openai/gpt-oss-120b?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_120b"
      model_configuration: "reasoning=$reasoning"
    provider: baseten
    model_id: openai/gpt-oss-120b
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true
    cost:
      input_cost_per_1m: 0.10
      output_cost_per_1m: 0.50
      currency: USD
    default_params:
      reasoning_effort: $reasoning
