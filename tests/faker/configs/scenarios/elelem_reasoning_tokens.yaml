scenario: "elelem_reasoning_tokens"
description: "Test Elelem's reasoning token extraction from different response formats"
type: "conditional"

models:
  "reasoning-test":
    provider: faker
    capabilities:
      supports_json_mode: true
      supports_temperature: true
      supports_system: true

conditions:
  - check: "temperature_equal"
    value: 1.0
    response:
      type: "reasoning"
      content: "The answer is 42 because of complex reasoning."
      tokens:
        input: 50
        output: 30
        reasoning: 15  # OpenAI-style nested reasoning tokens
      reasoning_format: "openai"

  - check: "temperature_equal"
    value: 0.8
    response:
      type: "reasoning"
      content: "After thinking about this problem, the solution is clear."
      tokens:
        input: 50
        output: 35
        reasoning: 20
      reasoning_format: "groq"

  - check: "temperature_equal"
    value: 0.6
    response:
      type: "reasoning"
      content: "<think>Let me analyze this step by step. First, I need to understand the problem. Then I can work through the solution methodically.</think>The final answer is 42."
      tokens:
        input: 50
        output: 45
        reasoning: 25  # Should be extracted from <think> tags

  - check: "temperature_equal"
    value: 0.4
    response:
      type: "reasoning"
      content: "# Reasoning\n\nThis is my thought process:\n1. Analyze the problem\n2. Consider alternatives\n3. Choose best solution\n\n# Answer\n\nThe answer is 42."
      tokens:
        input: 50
        output: 60
        reasoning: 35  # Should be extracted from reasoning sections

default_response:
  type: "reasoning"
  content: "Simple answer without reasoning tokens."
  tokens:
    input: 50
    output: 20
    reasoning: 0