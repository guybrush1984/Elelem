# Virtual models that reference multiple providers for resilience
# These models automatically failover between providers based on candidate configuration
#
# Optional routing config for benchmark-based candidate ordering:
#   routing:
#     speed_weight: 1.0        # Exponent for speed in value score (>1 favors speed, <1 favors cost)
#     min_tokens_per_sec: 0.0  # Minimum speed requirement (falls back to YAML order if none qualify)

models:
  # DEEPSEEK 3.2 VARIANTS
  # Non-reasoning: baseten, gmi, novita, deepinfra, parasail (fireworks/ollama always reason)
  "virtual:deepseek-v3.2-cheap":
    metadata:
      model_reference: "deepseek_v32"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "deepinfra:deepseek/deepseek-3.2"
      - model: "gmi:deepseek/deepseek-3.2"
      - model: "novita:deepseek/deepseek-3.2"
      - model: "baseten:deepseek/deepseek-3.2"
      - model: "parasail:deepseek/deepseek-3.2"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.2-quick":
    metadata:
      model_reference: "deepseek_v32"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "gmi:deepseek/deepseek-3.2"
      - model: "baseten:deepseek/deepseek-3.2"
      - model: "novita:deepseek/deepseek-3.2"
      - model: "deepinfra:deepseek/deepseek-3.2"
      - model: "parasail:deepseek/deepseek-3.2"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  # Reasoning: all providers support reasoning
  "virtual:deepseek-v3.2-cheap?reasoning":
    metadata:
      model_reference: "deepseek_v32"
      model_configuration: "reasoning"
    timeout: 300s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "ollama:deepseek/deepseek-3.2?reasoning"
        priority: always_first
      - model: "gmi:deepseek/deepseek-3.2?reasoning"
      - model: "deepinfra:deepseek/deepseek-3.2?reasoning"
      - model: "novita:deepseek/deepseek-3.2?reasoning"
      - model: "baseten:deepseek/deepseek-3.2?reasoning"
      - model: "fireworks:deepseek/deepseek-3.2?reasoning"
      - model: "parasail:deepseek/deepseek-3.2?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.2-quick?reasoning":
    metadata:
      model_reference: "deepseek_v32"
      model_configuration: "reasoning"
    timeout: 300s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "ollama:deepseek/deepseek-3.2?reasoning"
        priority: always_first
      - model: "fireworks:deepseek/deepseek-3.2?reasoning"
      - model: "gmi:deepseek/deepseek-3.2?reasoning"
      - model: "novita:deepseek/deepseek-3.2?reasoning"
      - model: "deepinfra:deepseek/deepseek-3.2?reasoning"
      - model: "baseten:deepseek/deepseek-3.2?reasoning"
      - model: "parasail:deepseek/deepseek-3.2?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  # DEEPSEEK 3.1 VARIANTS
  "virtual:deepseek-v3.1-cheap":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "ollama:deepseek/deepseek-3.1"
        priority: always_first
      - model: "gmi:deepseek/deepseek-3.1"
      - model: "deepinfra:deepseek/deepseek-3.1"
      - model: "novita:deepseek/deepseek-3.1"
      - model: "fireworks:deepseek/deepseek-3.1"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-quick":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "fireworks:deepseek/deepseek-3.1"
      - model: "gmi:deepseek/deepseek-3.1"
      - model: "novita:deepseek/deepseek-3.1"
      - model: "deepinfra:deepseek/deepseek-3.1"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-cheap?reasoning":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "ollama:deepseek/deepseek-3.1?reasoning"
        priority: always_first
      - model: "deepinfra:deepseek/deepseek-3.1?reasoning"
      - model: "fireworks:deepseek/deepseek-3.1?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-quick?reasoning":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "ollama:deepseek/deepseek-3.1?reasoning"
      - model: "fireworks:deepseek/deepseek-3.1?reasoning"
      - model: "deepinfra:deepseek/deepseek-3.1?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  # DEEPSEEK 3.1-TERMINUS VARIANTS
  "virtual:deepseek-v3.1-terminus-cheap":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "deepinfra:deepseek/deepseek-3.1-terminus"
      - model: "novita:deepseek/deepseek-3.1-terminus"
      - model: "fireworks:deepseek/deepseek-3.1-terminus"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-terminus-quick":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "fireworks:deepseek/deepseek-3.1-terminus"
      - model: "novita:deepseek/deepseek-3.1-terminus"
      - model: "deepinfra:deepseek/deepseek-3.1-terminus"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-terminus-cheap?reasoning":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "deepinfra:deepseek/deepseek-3.1-terminus?reasoning"
      - model: "novita:deepseek/deepseek-3.1-terminus"
      - model: "fireworks:deepseek/deepseek-3.1-terminus?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  "virtual:deepseek-v3.1-terminus-quick?reasoning":
    metadata:
      model_reference: "deepseek_v31"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "fireworks:deepseek/deepseek-3.1-terminus?reasoning"
      - model: "novita:deepseek/deepseek-3.1-terminus"
      - model: "deepinfra:deepseek/deepseek-3.1-terminus?reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model_reference, never reordered

  # OPENAI GPT-OSS 20B VARIANTS
  "virtual:gpt-oss-20b-cheap?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_20b"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "gmi:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "novita:openai/gpt-oss-20b?reasoning=$reasoning"
      # Fireworks excluded: streaming bug with large JSON responses (GPT-OSS model issue)
      - model: "groq:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  "virtual:gpt-oss-20b-quick?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_20b"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "groq:openai/gpt-oss-20b?reasoning=$reasoning"
      # Fireworks excluded: streaming bug with large JSON responses (GPT-OSS model issue)
      - model: "gmi:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "novita:openai/gpt-oss-20b?reasoning=$reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  # OPENAI GPT-OSS 120B VARIANTS
  "virtual:gpt-oss-120b-cheap?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_120b"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "ollama:openai/gpt-oss-120b?reasoning=$reasoning"
        priority: always_first
      - model: "gmi:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "baseten:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-120b-turbo?reasoning=$reasoning"
      - model: "novita:openai/gpt-oss-120b?reasoning=$reasoning"
      # Fireworks excluded: streaming bug with large JSON responses (GPT-OSS model issue)
      - model: "groq:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "cerebras:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "scaleway:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "parasail:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  "virtual:gpt-oss-120b-quick?reasoning=@[low,medium,high]":
    metadata:
      model_reference: "openai_gpt_oss_120b"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "ollama:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "cerebras:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "groq:openai/gpt-oss-120b?reasoning=$reasoning"
      # Fireworks excluded: streaming bug with large JSON responses (GPT-OSS model issue)
      - model: "gmi:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "baseten:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-120b-turbo?reasoning=$reasoning"
      - model: "novita:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "scaleway:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "parasail:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "deepinfra:openai/gpt-oss-120b?reasoning=$reasoning"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  # MISTRAL SMALL/MEDIUM VARIANTS
  "virtual:mistral-small-quick":
    metadata:
      model_reference: "mistral_small_32"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "mistral:mistral/mistral-small"
      - model: "deepinfra:mistral/mistral-small"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  "virtual:mistral-small-cheap":
    metadata:
      model_reference: "mistral_small_32"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "deepinfra:mistral/mistral-small"
      - model: "mistral:mistral/mistral-small"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  # MISTRAL LARGE 3 VARIANTS
  "virtual:mistral-large-3":
    metadata:
      model_reference: "mistral_large_3"
    timeout: 120s
    candidates:
      - model: "mistral:mistral/mistral-large-3"
        priority: always_first
      - model: "virtual:deepseek-v3.2-cheap"  # Fallback: expands to all DeepSeek 3.2 candidates
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Ultimate fallback

  # KIMI K2 VARIANTS
  "virtual:kimi-k2-quick":
    metadata:
      model_reference: "moonshot_kimi_k2"
    timeout: 120s
    routing:
      speed_weight: 2.0  # Favor speed
    candidates:
      - model: "ollama:moonshotai/kimi-k2-instruct"
      - model: "groq:moonshotai/kimi-k2-instruct"
      - model: "fireworks:moonshotai/kimi-k2-instruct"
      - model: "parasail:moonshotai/kimi-k2-instruct"
      - model: "deepinfra:moonshotai/kimi-k2-instruct"
      - model: "novita:moonshotai/kimi-k2-instruct"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered

  "virtual:kimi-k2-cheap":
    metadata:
      model_reference: "moonshot_kimi_k2"
    timeout: 120s
    routing:
      speed_weight: 0.3  # Strongly favor cost (3x weight vs speed)
    candidates:
      - model: "ollama:moonshotai/kimi-k2-instruct"
        priority: always_first
      - model: "deepinfra:moonshotai/kimi-k2-instruct"
      - model: "parasail:moonshotai/kimi-k2-instruct"
      - model: "novita:moonshotai/kimi-k2-instruct"
      - model: "fireworks:moonshotai/kimi-k2-instruct"
      - model: "groq:moonshotai/kimi-k2-instruct"
      - model: "openai:openai/gpt-4.1"
        priority: always_last  # Fallback: different model, never reordered
